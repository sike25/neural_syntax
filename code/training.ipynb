{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef29308b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eea0d5",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9007122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of objects in each world\n",
    "WORLD_SIZE = 5\n",
    "\n",
    "# the dimension of the feature vector of each object\n",
    "# as produced by the encoder\n",
    "OBJECT_FEATURE_DIMENSION = 6\n",
    "\n",
    "# the dimension of the speaker's generated vector representation\n",
    "REPRESENTATION_DIMENSION = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9eac37",
   "metadata": {},
   "source": [
    "### Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2506cfd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after class definition on line 1 (754396681.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after class definition on line 1\n"
     ]
    }
   ],
   "source": [
    "class ObjectEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes a single (3x3) Object into a feature vector\n",
    "    Input     : [[001][100][010]] or Purple-Cirle-No-Outline. See metadata.json\n",
    "    InputSize : (3,3)\n",
    "    OutputSize: (OBJECT_FEATURE_DIMENSION)\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dimension=OBJECT_FEATURE_DIMENSION):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=2)  # → (batch, 4, 2, 2)\n",
    "            nn.Flatten()                    # → (batch, 16)\n",
    "            nn.Linear(16, output_dimension)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c08f8f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2709159329.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    def __init__(self, output_dimension):\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class Speaker(nn.Module):\n",
    "    \"\"\"\n",
    "    Transforms the encoded objects in world W + boolean inclusion mask for target subset X \n",
    "    into a representative vector\n",
    "    \n",
    "    InputSize : (WORLD_SIZE * OBJECT_FEATURE_DIMENSION) + WORLD_SIZE)\n",
    "    OutputSize: (REPRESENTATION_DIMENSION)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dimension=(WORLD_SIZE * OBJECT_FEATURE_DIMENSION) + WORLD_SIZE, \n",
    "                 output_dimension=REPRESENTATION_DIMENSION):\n",
    "        super().__init__()\n",
    "        self.speaker_net = nn.Sequential(\n",
    "            nn.Linear(input_dimension, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.RrLU(),\n",
    "            nn.Linear(32, output_dimension)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.speaker_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e08bca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mListener\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Listener(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes the resultant vector from the speaker, along with W_i (an element of the world W)\n",
    "    and predicts whether this element belongs to X, the target subset.\n",
    "    \n",
    "    InputSize : (REPRESENTATION_DIMENSION + OBJECT_FEATURE_DIMENSION)\n",
    "    OutputSize: (1) -> logit for binary classification\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dimension=REPRESENTATION_DIMENSION + OBJECT_FEATURE_DIMENSION, \n",
    "                 output_dimension=1):\n",
    "        super().__init__():\n",
    "            self.listener_net = nn.Sequential(\n",
    "                nn.Linear(input_dimension, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 8),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(8, output_dimension)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.listener_net(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4acbc1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1308499078.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def SpeakerListenerSystem(nn.Module):\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def SpeakerListenerSystem(nn.Module):\n",
    "    \"\"\"\n",
    "    The end-to-end system.\n",
    "    Encoder -> Speaker -> Listener\n",
    "    \"\"\"\n",
    "    def __init__(self, world_size, feature_dimension, representation_dimension):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.world_size               = world_size\n",
    "        self.feature_dimension        = feature_dimension\n",
    "        self.representation_dimension = representation_dimension\n",
    "        \n",
    "        speaker_input_size  = (self.world_size * self.feature_dimension) + self.world_size\n",
    "        listener_input_size = self.representation_dimension + self.feature_dimension\n",
    "        \n",
    "        self.encoder  = ObjectEncoder(output_dimension=self.feature_dimension)\n",
    "        self.speaker  = Speaker(input_dimension=speaker_input_size, output_dimemsion=representation_dimension)\n",
    "        self.listener = Listener(input_dimension=listener_input_size, output_dimension=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, W, X_mask):\n",
    "        \"\"\"\n",
    "        W:      A batch of worlds. \n",
    "                Each world is a set of (3x3) objects.\n",
    "                Tensor of shape (batch_size, world_size, 3, 3).\n",
    "                \n",
    "        X_mask: A batch of boolean masks\n",
    "                Each value indicated whether the object at that index in W\n",
    "                is included in the target subset X\n",
    "                Tensor of shape (batch_size, world_size)\n",
    "        \"\"\"\n",
    "        batch_size = W.shape[0]\n",
    "        \n",
    "        ### STEP 1: Encode all objects in the world\n",
    "        # Reshape for batch processing ny the encoder (B, 5, 3, 3) -> (B*5, 1, 3, 3)\n",
    "        W_flat = W.view(-1, 1, 3, 3)\n",
    "        # Get features for all objects: (B*5, feature_dim)\n",
    "        object_features_flat = self.encoder(W_flat)\n",
    "        # Reshape back to per-batch item:(B, 5, feature_dim)\n",
    "        object_features = object_features_flat.view(batch_size, self.world_size, self.feature_dimension)\n",
    "        \n",
    "        \n",
    "        ### STEP 2: Assemble the inputs to the speaker model\n",
    "        # Flatten object features: (B, 5, feature_dim) -> (B, 5*feature_dim)\n",
    "        V_W =  object_features.view(batch_size, -1)\n",
    "        # Create the indicator mask\n",
    "        M_X = X_mask.float()\n",
    "        # Concatenate features and mask\n",
    "        speaker_input = torch.cat([V_W, M_X], dim=1)\n",
    "        \n",
    "        \n",
    "        ### STEP 3: Speaker generates neuralese\n",
    "        # representation has shape (B, representation_dimension)\n",
    "        representation = self.speaker(speaker_input)\n",
    "        \n",
    "        ### STEP 4: Prepare the listener's input\n",
    "        # The listener needs to pair the speaker's representation with each object feature\n",
    "        # Expand the representation to match the number of objects\n",
    "        # (B, rep_dim) -> (B, 1, rep_dim) -> (B, world_size, rep_dim)\n",
    "        r_expanded = representation.unsqueeze(1).repeat(1, self.world_size, 1)\n",
    "        # Concatenate with object features: (B, 5, rep_dim) + (B, 5, feature_dim)\n",
    "        listener_input = torch.cat([r_expanded, object_features], dim=2)\n",
    "        \n",
    "        \n",
    "        ### STEP 5: Shuffle inputs to the listener\n",
    "        # This is to avoid the speaker simply learning to tell the listener about X_mask\n",
    "        # without learning anything about the objects in X themselves.\n",
    "        # Create a random permutation for each item in the batch\n",
    "        shuffled_indices = [torch.randperm(self.world_size) for _ in range(batch_size)]\n",
    "        # Apply the shuffle\n",
    "        shuffled_input   = torch.stack([features[p] for features, p in zip(listener_input, shuffled_indices)])\n",
    "        shuffled_labels  = torch.stack([labels[p]   for labels,   p in zip(X_mask, shuffled_indices)])\n",
    "        \n",
    "        \n",
    "        ### STEP 6: Listener makes an inclusion prediction for each object\n",
    "        # Reshape for batch processing by the listener\n",
    "        # (B, 5, rep_dim + feature_dim) -> (B*5, rep_dim + feature_dim)\n",
    "        listener_input_flat = shuffled_input.view(-1, self.representation_dimension + self.feature_dimension)\n",
    "        # Get predictions (logits) -> (B*5, 1)\n",
    "        predictions_flat = self.listener(listener_input_flat)\n",
    "        # Reshape back to (B, world_size)\n",
    "        predictions = predictions_flat.view(batch_size, self.world_size)\n",
    "        \n",
    "        return predictions, shuffled_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d93985",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eee349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, world_size):\n",
    "    \"\"\"Generates a batch of dummy data\"\"\"\n",
    "    \n",
    "    # Random 3x3 objects for the world\n",
    "    W_batch = torch.rand(batch_size, world_size, 3, 3)\n",
    "    \n",
    "    # For each item, randomly decide what objects are in the target set X\n",
    "    X_mask_batch = []\n",
    "    for _ in range(batch_size):\n",
    "        X_mask = [random.randint(0, 1) for _ in range(world_size)]\n",
    "        X_mask_batch.append(indices)\n",
    "        \n",
    "    return W_batch, X_mask_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d08dda",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training\n",
    "BATCH_SIZE    = 2 # 32\n",
    "NUM_EPOCHS    = 1 # 2000\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7638d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the system\n",
    "model = SpeakerListenerSystem(\n",
    "    world_size               = WORLD_SIZE,\n",
    "    feature_dimension        = OBJECT_FEATURE_DIMENSION,\n",
    "    representation_dimension = REPRESENTATION_DIMENSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ea6a9c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (7352943.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 23\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# Use BCEWithLogitsLoss because our model outputs raw logits (more stable)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # Generate a new batch of data\n",
    "    W, X_mask = generate_batch(BATCH_SIZE, WORLD_SIZE)\n",
    "    \n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()\n",
    "    y_logits = model(W, X_mask)\n",
    "    \n",
    "    # Calculate loss - (X_mask serves as the ground truth)\n",
    "    loss = criterion(y_logits, X_mask)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        # Calculate accuracy for monitoring\n",
    "        preds    = torch.sigmoid(y_logits) > 0.5\n",
    "        accuracy = (preds.float() == X_mask).float().mean() \n",
    "        print(f\"Epoch [{epoch}/{NUM_EPOCHS}], Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}\")\n",
    "        \n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc4918",
   "metadata": {},
   "source": [
    "### Example Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9e02bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running a test example ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Running a test example ---\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    W_test, X_test_mask = generate(1, WORLD_SIZE)\n",
    "    \n",
    "    print(\"Test World W has 5 objects.\")\n",
    "    print(f\"Target set X is selected by mask: {X_test_mask[0]}\")\n",
    "    print(f\"Ground truth vector: {X_test_mask.numpy().flatten()}\")\n",
    "    \n",
    "    y_test_logits = model(W_test, X_test_mask)\n",
    "    y_test_probs  = torch.sigmoid(y_test_logits)\n",
    "    \n",
    "    print(f\"Model prediction (probabilities): {y_test_probs.numpy().flatten()}\")\n",
    "    print(f\"Final prediction (rounded): {[round(p, 2) for p in y_test_probs.numpy().flatten()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ed150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
