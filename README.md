### Neural Babel: What Do Neural Networks Talk About?
#### October, 2025

This project was published in the UniReps Workshop at NeurIPS '25.     
See https://unireps.org/blog/2025/neural-babel/!


In this project, two neural networks—a "speaker" and a "listener"—learned to work together by inventing their own code to describe which objects in a scene matched certain rules (like "all the blue things" or "circles or triangles"). A "translator" network then tries decode this AI-invented language back into English, achieving about 84% accuracy when the translations were grammatically valid. 

The details and results are laid out in [neural_syntax/blog_post.md](https://github.com/sike25/neural_syntax/blob/main/blog_post.md).

This work was built of Andreas and Klein's 2017 paper Analogs of Linguistic Structure in Deep Representations. 
You can find an explainer of it at: 
[interpretability/andreas.md](https://github.com/sike25/interpretability/blob/main/07_Complex_Explanations/andreas.md)




